{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ABMHub/NLP/blob/main/preprocess.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Inicialização"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OUq4cEL-o-Ue",
        "outputId": "15fb418a-7a3f-4159-8249-f3619172c90e"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to\n",
            "[nltk_data]     C:\\Users\\lucas\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 28,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import re\n",
        "import nltk\n",
        "from nltk.stem.porter import *\n",
        "from typing import List\n",
        "\n",
        "nltk.download(\"stopwords\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "yNEYu3kQrz7j",
        "outputId": "50b4e01c-a2ce-48ec-ad56-2e8ffffa1ea3"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>ID</th>\n",
              "      <th>emotion</th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>264183816548130816</td>\n",
              "      <td>positive</td>\n",
              "      <td>Gas by my house hit $3.39!!!! I\\u2019m going t...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>263405084770172928</td>\n",
              "      <td>negative</td>\n",
              "      <td>Theo Walcott is still shit\\u002c watch Rafa an...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>262163168678248449</td>\n",
              "      <td>negative</td>\n",
              "      <td>its not that I\\u2019m a GSP fan\\u002c i just h...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>264249301910310912</td>\n",
              "      <td>negative</td>\n",
              "      <td>Iranian general says Israel\\u2019s Iron Dome c...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>262682041215234048</td>\n",
              "      <td>neutral</td>\n",
              "      <td>Tehran\\u002c Mon Amour: Obama Tried to Establi...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9679</th>\n",
              "      <td>103158179306807296</td>\n",
              "      <td>positive</td>\n",
              "      <td>RT @MNFootNg It's monday and Monday Night Foot...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9680</th>\n",
              "      <td>103157324096618497</td>\n",
              "      <td>positive</td>\n",
              "      <td>All I know is the road for that Lomardi start ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9681</th>\n",
              "      <td>100259220338905089</td>\n",
              "      <td>neutral</td>\n",
              "      <td>All Blue and White fam, we r meeting at Golden...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9682</th>\n",
              "      <td>104230318525001729</td>\n",
              "      <td>positive</td>\n",
              "      <td>@DariusButler28   Have a great game agaist Tam...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9683</th>\n",
              "      <td>100461938533863424</td>\n",
              "      <td>negative</td>\n",
              "      <td>I'm pisseeedddd that I missed Kid Cudi's show ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>9684 rows × 3 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                      ID   emotion  \\\n",
              "0     264183816548130816  positive   \n",
              "1     263405084770172928  negative   \n",
              "2     262163168678248449  negative   \n",
              "3     264249301910310912  negative   \n",
              "4     262682041215234048   neutral   \n",
              "...                  ...       ...   \n",
              "9679  103158179306807296  positive   \n",
              "9680  103157324096618497  positive   \n",
              "9681  100259220338905089   neutral   \n",
              "9682  104230318525001729  positive   \n",
              "9683  100461938533863424  negative   \n",
              "\n",
              "                                                   text  \n",
              "0     Gas by my house hit $3.39!!!! I\\u2019m going t...  \n",
              "1     Theo Walcott is still shit\\u002c watch Rafa an...  \n",
              "2     its not that I\\u2019m a GSP fan\\u002c i just h...  \n",
              "3     Iranian general says Israel\\u2019s Iron Dome c...  \n",
              "4     Tehran\\u002c Mon Amour: Obama Tried to Establi...  \n",
              "...                                                 ...  \n",
              "9679  RT @MNFootNg It's monday and Monday Night Foot...  \n",
              "9680  All I know is the road for that Lomardi start ...  \n",
              "9681  All Blue and White fam, we r meeting at Golden...  \n",
              "9682  @DariusButler28   Have a great game agaist Tam...  \n",
              "9683  I'm pisseeedddd that I missed Kid Cudi's show ...  \n",
              "\n",
              "[9684 rows x 3 columns]"
            ]
          },
          "execution_count": 29,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df = pd.read_csv(\"https://raw.githubusercontent.com/viniciusrpb/cic0269_natural_language_processing/main/corpus_tweets/twitter-2013train-A.txt\", sep=\"\\t\", names=[\"ID\", \"emotion\", \"text\"])\n",
        "df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_dSA7CcV9-4k"
      },
      "source": [
        "Trecho de código que eu fico re-executando para conseguir ler tweets randomicamente"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "jFi6mKTP87S8",
        "outputId": "f21b73f9-0494-40a5-a445-6c70619bc584"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'Tonight was just a preview of what Bob Jones will do to Austin at tomorrow\\\\u2019s vball game. VBC. Noon. #BeThere #BJP'"
            ]
          },
          "execution_count": 30,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import random\n",
        "df.text[random.randrange(0, 9683)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KvRRbiOtvv_M",
        "outputId": "ec550bea-a490-4239-a05c-ce1333cab94e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0     Gas by my house hit $3.39!!!! I\\u2019m going t...\n",
              "1     Theo Walcott is still shit\\u002c watch Rafa an...\n",
              "2     its not that I\\u2019m a GSP fan\\u002c i just h...\n",
              "3     Iranian general says Israel\\u2019s Iron Dome c...\n",
              "4     Tehran\\u002c Mon Amour: Obama Tried to Establi...\n",
              "5     I sat through this whole movie just for Harry ...\n",
              "6     with J Davlar 11th. Main rivals are team Polan...\n",
              "7     Talking about ACT\\u2019s && SAT\\u2019s\\u002c d...\n",
              "8     Why is \\\"\"Happy Valentines Day\\\"\" trending? It...\n",
              "9     They may have a SuperBowl in Dallas\\u002c but ...\n",
              "10    Im bringing the monster load of candy tomorrow...\n",
              "11    Apple software\\u002c retail chiefs out in over...\n",
              "12    @oluoch @victor_otti @kunjand I just watched i...\n",
              "13    One of my best 8th graders Kory was excited af...\n",
              "14    #Livewire Nadal confirmed for Mexican Open in ...\n",
              "15    @MsSheLahY I didnt want to just pop up... but ...\n",
              "16    @Alyoup005 @addicted2haley hmmmm  November is ...\n",
              "17    #Iran US delisting MKO from global terrorists ...\n",
              "18    @JackStirling serge is amazing... like hes act...\n",
              "19    @HatersgonnHate_ @HAMlikeHussain @Ramythe3lite...\n",
              "Name: text, dtype: object"
            ]
          },
          "execution_count": 31,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "text_series = df.text\n",
        "text_series[0:20]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Teoria / Aula"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "88Kyq0D0wv5b",
        "outputId": "f9628496-3864-4352-f349-299dd8e86c76"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'gas by my house hit $3.39!!!! i\\\\u2019m going to chapel hill on sat. :)'"
            ]
          },
          "execution_count": 32,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "primeira = text_series[0]\n",
        "primeira_lower = primeira.lower()\n",
        "primeira_lower"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JYVxj-t-zjiU"
      },
      "source": [
        "Tokenização. Separar a frase em palavras (tokens) e limpar o texto de caracteres inúteis."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nKTp4eBByppc",
        "outputId": "da684f13-0c90-4f56-e695-e7dfd2bef0c9"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['gas',\n",
              " 'by',\n",
              " 'my',\n",
              " 'house',\n",
              " 'hit',\n",
              " 'i\\\\u',\n",
              " 'm',\n",
              " 'going',\n",
              " 'to',\n",
              " 'chapel',\n",
              " 'hill',\n",
              " 'on',\n",
              " 'sat',\n",
              " ':)']"
            ]
          },
          "execution_count": 33,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tokens = re.split(r'[ !@#.,?\\d$]+', primeira_lower)\n",
        "tokens"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uk8O_V-mzt-c"
      },
      "source": [
        "Stopwords são palavras que não acrescentam em nada no texto, ou seja, palavras de ligação, artigos... etc."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "AaV4BWbxz3Pa"
      },
      "outputs": [],
      "source": [
        "# stopwrd_list = ['and', 'by', 'on', 'to', 'or', 'in']\n",
        "# stopwrd_list\n",
        "stopword_list_nltk = nltk.corpus.stopwords.words(\"english\")\n",
        "# stopword_list_nltk"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6bf7QTML5zik",
        "outputId": "78ed7b5c-a6b0-4898-e5de-f1109ded8ea3"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['gas', 'house', 'hit', 'i\\\\u', 'going', 'chapel', 'hill', 'sat', ':)']"
            ]
          },
          "execution_count": 35,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "clean_words = []\n",
        "for token in tokens:\n",
        "    if token not in stopword_list_nltk:\n",
        "      clean_words.append(token)\n",
        "\n",
        "clean_words"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "25jDvoPO3R9M"
      },
      "source": [
        "Stemmer é um algoritmo que pega as palavras e reduz elas ao seu radical, para que flexões de gênero, número, etc não façam diferença alguma. Afinal, todas as flexões significam a mesma coisa."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wQnkg-Ld3fF7",
        "outputId": "8a14e83d-24f7-4f93-fa9f-2242bb7bf6d0"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['ga',\n",
              " 'by',\n",
              " 'my',\n",
              " 'hous',\n",
              " 'hit',\n",
              " 'i\\\\u',\n",
              " 'm',\n",
              " 'go',\n",
              " 'to',\n",
              " 'chapel',\n",
              " 'hill',\n",
              " 'on',\n",
              " 'sat',\n",
              " ':)']"
            ]
          },
          "execution_count": 36,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "stemmer = PorterStemmer()\n",
        "stem = [stemmer.stem(t) for t in tokens]\n",
        "stem"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RVQy1ZOh4pCD"
      },
      "source": [
        "# Atividade"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "Gh-2x5lN4n9c"
      },
      "outputs": [],
      "source": [
        "mp = {\n",
        "    \":)\": \"happy\",\n",
        "    \"(:\": \"happy\",\n",
        "    \":D\": \"happy\",\n",
        "    \":(\": \"sad\",\n",
        "    \"):\": \"sad\",\n",
        "    \"D:\": \"sad\",\n",
        "    \"xd\": \"fail\",\n",
        "    \"gr8\": \"great\",\n",
        "    \"lol\": \"laugh\",\n",
        "    \"plz\": \"please\",\n",
        "    \"m8\": \"mate\",\n",
        "    \"idc\": \"i don't care\",\n",
        "    \"imo\": \"in my opinion\",\n",
        "    \"rt\": \"\"\n",
        "}\n",
        "\n",
        "def processamento_de_texto(texto : str) -> List[str]:\n",
        "  \"\"\"Função para fazer o pré-processamento de textos. A função segue a seguinte ordem:\n",
        "  Transforma string em lower case\n",
        "  Separa a string em tokens, removendo caracteres especiais\n",
        "  Substitui algumas palavras e acrônimos pelos seus significados\n",
        "\n",
        "\n",
        "  Args:\n",
        "      texto (str): _description_\n",
        "\n",
        "  Returns:\n",
        "      List[str]: Retorna lista de tweets com palavras processadas\n",
        "  \"\"\"\n",
        "  lower_case = texto.lower()\n",
        "  \n",
        "  tokens = re.split(r'[ !@#.,?\\d$\\\\/:~\"\\'\\(\\)\\+\\-;_\\*<>\\[\\]\\{\\}\\|\\=\\&\\^\\´\\`\\%]+', lower_case)\n",
        "  for i in range(len(tokens)): # substituicao com dicionario\n",
        "    if tokens[i] in mp:\n",
        "      tokens[i] = mp[tokens[i]]\n",
        "  stopword_list_nltk = nltk.corpus.stopwords.words(\"english\")\n",
        "  clean_words = []\n",
        "  for token in tokens: # remocao de stop words\n",
        "    if token not in stopword_list_nltk:\n",
        "      clean_words.append(token)\n",
        "  stemmer = PorterStemmer()\n",
        "  stem = [stemmer.stem(t) for t in clean_words]\n",
        "  return \" \".join(stem)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Tweets pré-processados"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "LrrmuZDk7Gjb",
        "outputId": "99883b3d-017f-4e47-88d9-be0700dbcdc2"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>ID</th>\n",
              "      <th>emotion</th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>264183816548130816</td>\n",
              "      <td>positive</td>\n",
              "      <td>ga hous hit u go chapel hill sat</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>263405084770172928</td>\n",
              "      <td>negative</td>\n",
              "      <td>theo walcott still shit u c watch rafa johnni ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>262163168678248449</td>\n",
              "      <td>negative</td>\n",
              "      <td>u gsp fan u c hate nick diaz u wait februari</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>264249301910310912</td>\n",
              "      <td>negative</td>\n",
              "      <td>iranian gener say israel u iron dome u deal mi...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>262682041215234048</td>\n",
              "      <td>neutral</td>\n",
              "      <td>tehran u c mon amour obama tri establish tie m...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9679</th>\n",
              "      <td>103158179306807296</td>\n",
              "      <td>positive</td>\n",
              "      <td>mnfootng monday monday night footbal mind  lo...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9680</th>\n",
              "      <td>103157324096618497</td>\n",
              "      <td>positive</td>\n",
              "      <td>know road lomardi start tonight set record pre...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9681</th>\n",
              "      <td>100259220338905089</td>\n",
              "      <td>neutral</td>\n",
              "      <td>blue white fam r meet golden corral dinner ton...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9682</th>\n",
              "      <td>104230318525001729</td>\n",
              "      <td>positive</td>\n",
              "      <td>dariusbutl great game agaist tampa bay tonight</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9683</th>\n",
              "      <td>100461938533863424</td>\n",
              "      <td>negative</td>\n",
              "      <td>pisseeedddd miss kid cudi show dalla trend wor...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>9684 rows × 3 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                      ID   emotion  \\\n",
              "0     264183816548130816  positive   \n",
              "1     263405084770172928  negative   \n",
              "2     262163168678248449  negative   \n",
              "3     264249301910310912  negative   \n",
              "4     262682041215234048   neutral   \n",
              "...                  ...       ...   \n",
              "9679  103158179306807296  positive   \n",
              "9680  103157324096618497  positive   \n",
              "9681  100259220338905089   neutral   \n",
              "9682  104230318525001729  positive   \n",
              "9683  100461938533863424  negative   \n",
              "\n",
              "                                                   text  \n",
              "0                     ga hous hit u go chapel hill sat   \n",
              "1     theo walcott still shit u c watch rafa johnni ...  \n",
              "2         u gsp fan u c hate nick diaz u wait februari   \n",
              "3     iranian gener say israel u iron dome u deal mi...  \n",
              "4     tehran u c mon amour obama tri establish tie m...  \n",
              "...                                                 ...  \n",
              "9679   mnfootng monday monday night footbal mind  lo...  \n",
              "9680  know road lomardi start tonight set record pre...  \n",
              "9681  blue white fam r meet golden corral dinner ton...  \n",
              "9682    dariusbutl great game agaist tampa bay tonight   \n",
              "9683  pisseeedddd miss kid cudi show dalla trend wor...  \n",
              "\n",
              "[9684 rows x 3 columns]"
            ]
          },
          "execution_count": 38,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "series = text_series.apply(processamento_de_texto)\n",
        "df.text = series\n",
        "df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Vocabulário"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0                   cuku\n",
              "1                 parent\n",
              "2                  tempt\n",
              "3        mountainsandsea\n",
              "4                    soy\n",
              "              ...       \n",
              "19539          jaggerhop\n",
              "19540          denverdeb\n",
              "19541          grupoedeb\n",
              "19542             streep\n",
              "19543             damian\n",
              "Name: Vocabulario, Length: 19544, dtype: object"
            ]
          },
          "execution_count": 39,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "st = set()\n",
        "def vocab(wrds):\n",
        "  for wrd in wrds.split():\n",
        "    st.add(wrd)\n",
        "\n",
        "series.apply(vocab)\n",
        "\n",
        "vocabulario = pd.Series(list(st),name=\"Vocabulario\")\n",
        "vocabulario.sort_values().to_csv(\"output.csv\", index=False)\n",
        "vocabulario"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyN3aPCmDZOWUichp3gyvvI3",
      "include_colab_link": true,
      "name": "Untitled1.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.9.13 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "vscode": {
      "interpreter": {
        "hash": "3c3fdd1ba6fa6149e0a55b9d95e637df8fd62d3a04e8e673bb353b444f545e4d"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
